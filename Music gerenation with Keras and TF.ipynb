{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got this python notebook from the following GitHub repo (https://github.com/subpath/Keras_music_gereration) as a way to test our baseline:\n",
    "\n",
    "### Music gereration firth Keras and TensorFlow backend\n",
    "\n",
    "Plan was simple:\n",
    "1. Read midi file, convert it to matrix of features\n",
    "2. Create simple model with Keras and LSTM to learn the pattern\n",
    "3. Use subsample of initial midi file as a input for model to generate pure art\n",
    "4. Save prediction from model to midi file\n",
    ".\n",
    ".\n",
    ".\n",
    "5. PROFIT\n",
    "\n",
    "<i> For disclamer: I've been using my old Dell Laptop with no GPU support</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "import os\n",
    "from mido import MidiFile, MidiTrack, Message\n",
    "from keras.layers import LSTM, Dense, Activation, Dropout, Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read midi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = MidiFile('Data/Bamboleo.mid')  \n",
    "# midis = []\n",
    "# print(os.listdir('Data'))\n",
    "# for file in os.listdir('Data'):\n",
    "#     if file.endswith(\".mid\"):\n",
    "#         print(file)\n",
    "#         midis.append(MidiFile('Data/' + file, clip=True))\n",
    "    #files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid = MidiFile('Data/abusowillieCol.mid', clip=True)\n",
    "\n",
    "# for i, track in enumerate(mid.tracks):\n",
    "#     print('Track {}: {}'.format(i, track.name))\n",
    "#     for msg in track:\n",
    "#         if msg.type == 'program_change':\n",
    "#             print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract notes sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = []\n",
    "\n",
    "def isPercussion(channel):\n",
    "    return channel in range(8, 17) or channel in range(35, 82)\n",
    "\n",
    "for msg in mid:\n",
    "#         if msg.type == 'program_change':\n",
    "#             print(msg)\n",
    "    if not msg.is_meta and msg.channel == 0 and msg.type == 'note_on':\n",
    "        data = msg.bytes()\n",
    "        notes.append(data[1])\n",
    "\n",
    "# for file in os.listdir('Data')[1:6]:\n",
    "#     mid = MidiFile('Data/' + file, clip=True)\n",
    "#     for msg in mid:\n",
    "# #         if msg.type == 'program_change':\n",
    "# #             print(msg)\n",
    "#         if not msg.is_meta and msg.channel == 0 and msg.type == 'note_on':\n",
    "#             data = msg.bytes()\n",
    "#             notes.append(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply min-max scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/StephanieBrito/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(np.array(notes).reshape(-1,1))\n",
    "notes = list(scaler.transform(np.array(notes).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare features for training and data subsample for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37 49]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# LSTM layers requires that data must have a certain shape\n",
    "# create list of lists fist\n",
    "#notes = [list(note) for note in notes]\n",
    "\n",
    "# subsample data for training and prediction\n",
    "X = []\n",
    "y = []\n",
    "# number of notes in a batch\n",
    "n_prev = 2\n",
    "for i in range(len(notes)-n_prev):\n",
    "    X.append(notes[i:i+n_prev])\n",
    "    y.append(notes[i+n_prev])\n",
    "# save a seed to do prediction later\n",
    "X_test = X[-300:]\n",
    "X = np.asarray(X[:-300])\n",
    "y = y[:-300]\n",
    "print(X[0])\n",
    "print(len(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "print(len(dummy_y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Made sequential model with several layers, use LSTM as it time dependent data\n",
    "\n",
    "I also whant to save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3427/3427 [==============================] - 5s 1ms/step - loss: 11.0551 - acc: 0.0986\n",
      "Epoch 2/20\n",
      "3427/3427 [==============================] - 1s 376us/step - loss: 10.4918 - acc: 0.1240\n",
      "Epoch 3/20\n",
      "3427/3427 [==============================] - 1s 368us/step - loss: 10.4631 - acc: 0.1161\n",
      "Epoch 4/20\n",
      "3427/3427 [==============================] - 1s 363us/step - loss: 10.4528 - acc: 0.1153\n",
      "Epoch 5/20\n",
      "3427/3427 [==============================] - 1s 384us/step - loss: 10.4457 - acc: 0.12140s -\n",
      "Epoch 6/20\n",
      "3427/3427 [==============================] - 2s 448us/step - loss: 10.4435 - acc: 0.1179\n",
      "Epoch 7/20\n",
      "3427/3427 [==============================] - 1s 417us/step - loss: 10.4431 - acc: 0.1191\n",
      "Epoch 8/20\n",
      "3427/3427 [==============================] - 1s 372us/step - loss: 10.4370 - acc: 0.1170\n",
      "Epoch 9/20\n",
      "3427/3427 [==============================] - 1s 377us/step - loss: 10.4346 - acc: 0.1176\n",
      "Epoch 10/20\n",
      "3427/3427 [==============================] - 1s 385us/step - loss: 10.4321 - acc: 0.1211\n",
      "Epoch 11/20\n",
      "3427/3427 [==============================] - 1s 386us/step - loss: 10.4342 - acc: 0.11820s - loss: 10.4494 -\n",
      "Epoch 12/20\n",
      "3427/3427 [==============================] - 1s 413us/step - loss: 10.4330 - acc: 0.1202\n",
      "Epoch 13/20\n",
      "3427/3427 [==============================] - 1s 411us/step - loss: 10.4282 - acc: 0.1226\n",
      "Epoch 14/20\n",
      "3427/3427 [==============================] - 1s 408us/step - loss: 10.4291 - acc: 0.1170\n",
      "Epoch 15/20\n",
      "3427/3427 [==============================] - 1s 386us/step - loss: 10.4278 - acc: 0.11880s - loss: 10.4236\n",
      "Epoch 16/20\n",
      "3427/3427 [==============================] - 1s 424us/step - loss: 10.4254 - acc: 0.1249\n",
      "Epoch 17/20\n",
      "3427/3427 [==============================] - 2s 463us/step - loss: 10.4248 - acc: 0.1182\n",
      "Epoch 18/20\n",
      "3427/3427 [==============================] - 3s 883us/step - loss: 10.4253 - acc: 0.1234\n",
      "Epoch 19/20\n",
      "3427/3427 [==============================] - 1s 426us/step - loss: 10.4232 - acc: 0.1237\n",
      "Epoch 20/20\n",
      "3427/3427 [==============================] - 1s 398us/step - loss: 10.4231 - acc: 0.12840s\n",
      "381/381 [==============================] - 1s 3ms/step\n",
      "Epoch 1/20\n",
      "3427/3427 [==============================] - 4s 1ms/step - loss: 9.0264 - acc: 0.0899\n",
      "Epoch 2/20\n",
      "3427/3427 [==============================] - 1s 377us/step - loss: 7.8559 - acc: 0.1085\n",
      "Epoch 3/20\n",
      "3427/3427 [==============================] - 2s 520us/step - loss: 7.8206 - acc: 0.1121\n",
      "Epoch 4/20\n",
      "3427/3427 [==============================] - 1s 416us/step - loss: 7.8097 - acc: 0.1185\n",
      "Epoch 5/20\n",
      "3427/3427 [==============================] - 1s 362us/step - loss: 7.8060 - acc: 0.1217\n",
      "Epoch 6/20\n",
      "3427/3427 [==============================] - 1s 384us/step - loss: 7.7986 - acc: 0.1138\n",
      "Epoch 7/20\n",
      "3427/3427 [==============================] - 1s 356us/step - loss: 7.8017 - acc: 0.1228\n",
      "Epoch 8/20\n",
      "3427/3427 [==============================] - 1s 412us/step - loss: 7.7972 - acc: 0.1182\n",
      "Epoch 9/20\n",
      "3427/3427 [==============================] - 1s 382us/step - loss: 7.7965 - acc: 0.1179\n",
      "Epoch 10/20\n",
      "3427/3427 [==============================] - 1s 365us/step - loss: 7.7962 - acc: 0.1144\n",
      "Epoch 11/20\n",
      "3427/3427 [==============================] - 1s 359us/step - loss: 7.7928 - acc: 0.1231\n",
      "Epoch 12/20\n",
      "3427/3427 [==============================] - 1s 372us/step - loss: 7.7957 - acc: 0.1226\n",
      "Epoch 13/20\n",
      "3427/3427 [==============================] - 1s 362us/step - loss: 7.7918 - acc: 0.1196\n",
      "Epoch 14/20\n",
      "3427/3427 [==============================] - 1s 404us/step - loss: 7.7920 - acc: 0.1138\n",
      "Epoch 15/20\n",
      "3427/3427 [==============================] - 1s 428us/step - loss: 7.7884 - acc: 0.1179\n",
      "Epoch 16/20\n",
      "3427/3427 [==============================] - 1s 358us/step - loss: 7.7843 - acc: 0.1214\n",
      "Epoch 17/20\n",
      "3427/3427 [==============================] - 1s 351us/step - loss: 7.7847 - acc: 0.1158\n",
      "Epoch 18/20\n",
      "3427/3427 [==============================] - 1s 350us/step - loss: 7.3928 - acc: 0.1170\n",
      "Epoch 19/20\n",
      "3427/3427 [==============================] - 1s 345us/step - loss: 2.7147 - acc: 0.1263\n",
      "Epoch 20/20\n",
      "3427/3427 [==============================] - 1s 343us/step - loss: 2.6638 - acc: 0.1293\n",
      "381/381 [==============================] - 1s 3ms/step\n",
      "Epoch 1/20\n",
      "3427/3427 [==============================] - 4s 1ms/step - loss: 15.2101 - acc: 0.0528\n",
      "Epoch 2/20\n",
      "3427/3427 [==============================] - 1s 421us/step - loss: 15.2080 - acc: 0.0525\n",
      "Epoch 3/20\n",
      "3427/3427 [==============================] - 2s 445us/step - loss: 15.2066 - acc: 0.05370s - loss:\n",
      "Epoch 4/20\n",
      "3427/3427 [==============================] - 1s 389us/step - loss: 14.7826 - acc: 0.0659\n",
      "Epoch 5/20\n",
      "3427/3427 [==============================] - 1s 384us/step - loss: 13.5417 - acc: 0.0989\n",
      "Epoch 6/20\n",
      "3427/3427 [==============================] - 1s 403us/step - loss: 13.5299 - acc: 0.1013\n",
      "Epoch 7/20\n",
      "3427/3427 [==============================] - 2s 508us/step - loss: 13.5260 - acc: 0.1053\n",
      "Epoch 8/20\n",
      "3427/3427 [==============================] - 1s 395us/step - loss: 13.5337 - acc: 0.0989\n",
      "Epoch 9/20\n",
      "3427/3427 [==============================] - 1s 387us/step - loss: 13.5297 - acc: 0.0992\n",
      "Epoch 10/20\n",
      "3427/3427 [==============================] - 1s 385us/step - loss: 13.5301 - acc: 0.1015\n",
      "Epoch 11/20\n",
      "3427/3427 [==============================] - 1s 388us/step - loss: 13.5278 - acc: 0.1039\n",
      "Epoch 12/20\n",
      "3427/3427 [==============================] - 1s 387us/step - loss: 13.5259 - acc: 0.1042\n",
      "Epoch 13/20\n",
      "3427/3427 [==============================] - 1s 364us/step - loss: 13.5270 - acc: 0.1013\n",
      "Epoch 14/20\n",
      "3427/3427 [==============================] - 1s 358us/step - loss: 13.5276 - acc: 0.1053\n",
      "Epoch 15/20\n",
      "3427/3427 [==============================] - 1s 357us/step - loss: 13.5306 - acc: 0.1015\n",
      "Epoch 16/20\n",
      "3427/3427 [==============================] - 1s 353us/step - loss: 13.5284 - acc: 0.1024\n",
      "Epoch 17/20\n",
      "3427/3427 [==============================] - 1s 355us/step - loss: 13.5251 - acc: 0.1030\n",
      "Epoch 18/20\n",
      "3427/3427 [==============================] - 1s 397us/step - loss: 13.5256 - acc: 0.10390s - loss: 13.5131 -\n",
      "Epoch 19/20\n",
      "3427/3427 [==============================] - 1s 425us/step - loss: 13.5242 - acc: 0.1039\n",
      "Epoch 20/20\n",
      "3427/3427 [==============================] - 1s 429us/step - loss: 13.5280 - acc: 0.1001\n",
      "381/381 [==============================] - 1s 3ms/step\n",
      "Epoch 1/20\n",
      "3427/3427 [==============================] - 4s 1ms/step - loss: 14.7620 - acc: 0.0484\n",
      "Epoch 2/20\n",
      "3427/3427 [==============================] - 1s 411us/step - loss: 13.8150 - acc: 0.0770\n",
      "Epoch 3/20\n",
      "3427/3427 [==============================] - 1s 411us/step - loss: 13.7674 - acc: 0.0805\n",
      "Epoch 4/20\n",
      "3427/3427 [==============================] - 1s 437us/step - loss: 13.7449 - acc: 0.0870\n",
      "Epoch 5/20\n",
      "3427/3427 [==============================] - 1s 383us/step - loss: 13.7347 - acc: 0.09020s - loss:\n",
      "Epoch 6/20\n",
      "3427/3427 [==============================] - 1s 387us/step - loss: 13.7237 - acc: 0.0966\n",
      "Epoch 7/20\n",
      "3427/3427 [==============================] - 1s 394us/step - loss: 13.7209 - acc: 0.1004\n",
      "Epoch 8/20\n",
      "3427/3427 [==============================] - 1s 431us/step - loss: 13.7193 - acc: 0.1010\n",
      "Epoch 9/20\n",
      "3427/3427 [==============================] - 2s 456us/step - loss: 13.7159 - acc: 0.1010\n",
      "Epoch 10/20\n",
      "3427/3427 [==============================] - 2s 464us/step - loss: 13.7181 - acc: 0.1007\n",
      "Epoch 11/20\n",
      "3427/3427 [==============================] - 2s 530us/step - loss: 13.7187 - acc: 0.1021\n",
      "Epoch 12/20\n",
      "3427/3427 [==============================] - 1s 418us/step - loss: 13.7159 - acc: 0.1024\n",
      "Epoch 13/20\n",
      "3427/3427 [==============================] - 1s 380us/step - loss: 13.7163 - acc: 0.1027\n",
      "Epoch 14/20\n",
      "3427/3427 [==============================] - 1s 363us/step - loss: 13.7163 - acc: 0.1021\n",
      "Epoch 15/20\n",
      "3427/3427 [==============================] - 1s 392us/step - loss: 13.7152 - acc: 0.1027\n",
      "Epoch 16/20\n",
      "3427/3427 [==============================] - 1s 362us/step - loss: 13.7131 - acc: 0.1015\n",
      "Epoch 17/20\n",
      "3427/3427 [==============================] - 1s 371us/step - loss: 13.7135 - acc: 0.10270s - loss: 1\n",
      "Epoch 18/20\n",
      "3427/3427 [==============================] - 1s 384us/step - loss: 13.7150 - acc: 0.09920s - loss: 13.7686 - acc: \n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3427/3427 [==============================] - 1s 345us/step - loss: 13.7131 - acc: 0.1024\n",
      "Epoch 20/20\n",
      "3427/3427 [==============================] - 1s 344us/step - loss: 13.7135 - acc: 0.1027\n",
      "381/381 [==============================] - 1s 3ms/step\n",
      "Epoch 1/20\n",
      "3427/3427 [==============================] - 4s 1ms/step - loss: 13.9255 - acc: 0.0849: 1s - loss: 14.0464\n",
      "Epoch 2/20\n",
      "3427/3427 [==============================] - 1s 402us/step - loss: 13.7073 - acc: 0.1004\n",
      "Epoch 3/20\n",
      "3427/3427 [==============================] - 1s 387us/step - loss: 13.7048 - acc: 0.0992\n",
      "Epoch 4/20\n",
      "3427/3427 [==============================] - 1s 376us/step - loss: 13.7027 - acc: 0.10300s - loss: 13.7417 - a\n",
      "Epoch 5/20\n",
      "3427/3427 [==============================] - 1s 371us/step - loss: 13.7007 - acc: 0.1036\n",
      "Epoch 6/20\n",
      "3427/3427 [==============================] - 1s 404us/step - loss: 13.6989 - acc: 0.1059\n",
      "Epoch 7/20\n",
      "3427/3427 [==============================] - 2s 450us/step - loss: 13.6977 - acc: 0.1062\n",
      "Epoch 8/20\n",
      "3427/3427 [==============================] - 1s 415us/step - loss: 13.6973 - acc: 0.1065\n",
      "Epoch 9/20\n",
      "3427/3427 [==============================] - 1s 385us/step - loss: 13.6957 - acc: 0.1065\n",
      "Epoch 10/20\n",
      "3427/3427 [==============================] - 1s 368us/step - loss: 13.6965 - acc: 0.1065\n",
      "Epoch 11/20\n",
      "3427/3427 [==============================] - 1s 378us/step - loss: 13.6961 - acc: 0.1074\n",
      "Epoch 12/20\n",
      "3427/3427 [==============================] - 1s 366us/step - loss: 13.6961 - acc: 0.1074\n",
      "Epoch 13/20\n",
      "3427/3427 [==============================] - 1s 380us/step - loss: 13.6964 - acc: 0.1065\n",
      "Epoch 14/20\n",
      "3427/3427 [==============================] - 2s 438us/step - loss: 13.6943 - acc: 0.1065\n",
      "Epoch 15/20\n",
      "3427/3427 [==============================] - 2s 493us/step - loss: 13.6957 - acc: 0.1071\n",
      "Epoch 16/20\n",
      "3427/3427 [==============================] - 1s 380us/step - loss: 13.6943 - acc: 0.1074\n",
      "Epoch 17/20\n",
      "3427/3427 [==============================] - 1s 386us/step - loss: 13.6948 - acc: 0.1074\n",
      "Epoch 18/20\n",
      "3427/3427 [==============================] - 1s 428us/step - loss: 9.6570 - acc: 0.1135\n",
      "Epoch 19/20\n",
      "3427/3427 [==============================] - 2s 472us/step - loss: 2.8340 - acc: 0.1196\n",
      "Epoch 20/20\n",
      "3427/3427 [==============================] - 2s 563us/step - loss: 2.7445 - acc: 0.1258\n",
      "381/381 [==============================] - 1s 3ms/step\n",
      "Epoch 1/20\n",
      "3427/3427 [==============================] - 5s 1ms/step - loss: 4.2236 - acc: 0.0896\n",
      "Epoch 2/20\n",
      "3427/3427 [==============================] - 1s 436us/step - loss: 2.8358 - acc: 0.0960\n",
      "Epoch 3/20\n",
      "3427/3427 [==============================] - 2s 533us/step - loss: 2.7286 - acc: 0.1045 0s - lo\n",
      "Epoch 4/20\n",
      "3427/3427 [==============================] - 2s 469us/step - loss: 2.6715 - acc: 0.1068 1s - loss: \n",
      "Epoch 5/20\n",
      "3427/3427 [==============================] - 2s 484us/step - loss: 2.6568 - acc: 0.1240\n",
      "Epoch 6/20\n",
      "3427/3427 [==============================] - 2s 503us/step - loss: 2.6550 - acc: 0.1266\n",
      "Epoch 7/20\n",
      "3427/3427 [==============================] - 2s 518us/step - loss: 2.6491 - acc: 0.1366\n",
      "Epoch 8/20\n",
      "3427/3427 [==============================] - 1s 433us/step - loss: 2.6491 - acc: 0.1325\n",
      "Epoch 9/20\n",
      "3427/3427 [==============================] - 1s 419us/step - loss: 2.6464 - acc: 0.1345\n",
      "Epoch 10/20\n",
      "3427/3427 [==============================] - 1s 426us/step - loss: 2.6450 - acc: 0.1339\n",
      "Epoch 11/20\n",
      "3427/3427 [==============================] - 2s 461us/step - loss: 2.6442 - acc: 0.1354\n",
      "Epoch 12/20\n",
      "3427/3427 [==============================] - 1s 436us/step - loss: 2.6441 - acc: 0.1369\n",
      "Epoch 13/20\n",
      "3427/3427 [==============================] - 1s 431us/step - loss: 2.6421 - acc: 0.1339\n",
      "Epoch 14/20\n",
      "3427/3427 [==============================] - 2s 599us/step - loss: 2.6427 - acc: 0.1313\n",
      "Epoch 15/20\n",
      "3427/3427 [==============================] - 2s 549us/step - loss: 2.6425 - acc: 0.1348\n",
      "Epoch 16/20\n",
      "3427/3427 [==============================] - 2s 531us/step - loss: 2.6419 - acc: 0.1363\n",
      "Epoch 17/20\n",
      "3427/3427 [==============================] - 1s 436us/step - loss: 2.6400 - acc: 0.1334\n",
      "Epoch 18/20\n",
      "3427/3427 [==============================] - 1s 420us/step - loss: 2.6394 - acc: 0.1316\n",
      "Epoch 19/20\n",
      "3427/3427 [==============================] - 1s 430us/step - loss: 2.6391 - acc: 0.1339 1s - loss: \n",
      "Epoch 20/20\n",
      "3427/3427 [==============================] - 2s 492us/step - loss: 2.6392 - acc: 0.1313\n",
      "381/381 [==============================] - 1s 3ms/step\n",
      "Epoch 1/20\n",
      "3427/3427 [==============================] - 4s 1ms/step - loss: 7.1254 - acc: 0.0995\n",
      "Epoch 2/20\n",
      "3427/3427 [==============================] - 1s 419us/step - loss: 5.3219 - acc: 0.1144\n",
      "Epoch 3/20\n",
      "3427/3427 [==============================] - 2s 532us/step - loss: 5.3218 - acc: 0.1167 0s - loss: 5.2120 - acc: 0.1 - ETA: 0s - loss: 5.2618 - acc: 0.10 - ETA: 0s - loss: 5.2279 - \n",
      "Epoch 4/20\n",
      "3427/3427 [==============================] - 1s 432us/step - loss: 5.3195 - acc: 0.1182\n",
      "Epoch 5/20\n",
      "3427/3427 [==============================] - 1s 398us/step - loss: 5.3184 - acc: 0.1182\n",
      "Epoch 6/20\n",
      "3427/3427 [==============================] - 1s 399us/step - loss: 5.3158 - acc: 0.1237\n",
      "Epoch 7/20\n",
      "3427/3427 [==============================] - 1s 394us/step - loss: 5.3175 - acc: 0.1179\n",
      "Epoch 8/20\n",
      "3427/3427 [==============================] - 1s 409us/step - loss: 5.3121 - acc: 0.1275\n",
      "Epoch 9/20\n",
      "3427/3427 [==============================] - 2s 465us/step - loss: 5.3160 - acc: 0.1182\n",
      "Epoch 10/20\n",
      "3427/3427 [==============================] - 1s 425us/step - loss: 5.3141 - acc: 0.1161\n",
      "Epoch 11/20\n",
      "3427/3427 [==============================] - 1s 419us/step - loss: 5.3089 - acc: 0.1214\n",
      "Epoch 12/20\n",
      "3427/3427 [==============================] - 1s 410us/step - loss: 5.3097 - acc: 0.1223\n",
      "Epoch 13/20\n",
      "3427/3427 [==============================] - 1s 412us/step - loss: 5.3103 - acc: 0.1156 0s - loss: 5.2943 - acc: 0.11\n",
      "Epoch 14/20\n",
      "3427/3427 [==============================] - 1s 411us/step - loss: 5.3105 - acc: 0.1217 0s - loss: 5.2241 \n",
      "Epoch 15/20\n",
      "3427/3427 [==============================] - 1s 400us/step - loss: 5.3091 - acc: 0.1164\n",
      "Epoch 16/20\n",
      "3427/3427 [==============================] - 1s 412us/step - loss: 5.3091 - acc: 0.1182\n",
      "Epoch 17/20\n",
      "3427/3427 [==============================] - 2s 524us/step - loss: 5.3094 - acc: 0.1214\n",
      "Epoch 18/20\n",
      "3427/3427 [==============================] - 2s 439us/step - loss: 5.3096 - acc: 0.1202\n",
      "Epoch 19/20\n",
      "3427/3427 [==============================] - 2s 453us/step - loss: 5.3083 - acc: 0.1226\n",
      "Epoch 20/20\n",
      "3427/3427 [==============================] - 2s 447us/step - loss: 5.3074 - acc: 0.1205\n",
      "381/381 [==============================] - 1s 3ms/step\n",
      "Epoch 1/20\n",
      "3427/3427 [==============================] - 4s 1ms/step - loss: 8.8435 - acc: 0.0966\n",
      "Epoch 2/20\n",
      "3427/3427 [==============================] - 2s 454us/step - loss: 7.2221 - acc: 0.1015 0s - loss: 7.2633 - acc: 0.1\n",
      "Epoch 3/20\n",
      "3427/3427 [==============================] - 2s 464us/step - loss: 7.1927 - acc: 0.1118\n",
      "Epoch 4/20\n",
      "3427/3427 [==============================] - 2s 463us/step - loss: 7.1914 - acc: 0.1085\n",
      "Epoch 5/20\n",
      "3427/3427 [==============================] - 2s 474us/step - loss: 7.1854 - acc: 0.1126 0s - loss: 7.1\n",
      "Epoch 6/20\n",
      "3427/3427 [==============================] - 2s 476us/step - loss: 7.1836 - acc: 0.1039A: 1s -\n",
      "Epoch 7/20\n",
      "3427/3427 [==============================] - 2s 455us/step - loss: 7.1808 - acc: 0.1150\n",
      "Epoch 8/20\n",
      "3427/3427 [==============================] - 2s 465us/step - loss: 7.1766 - acc: 0.1112\n",
      "Epoch 9/20\n",
      "3427/3427 [==============================] - 2s 490us/step - loss: 7.1759 - acc: 0.1167\n",
      "Epoch 10/20\n",
      "3427/3427 [==============================] - 2s 460us/step - loss: 7.1754 - acc: 0.1126\n",
      "Epoch 11/20\n",
      "3427/3427 [==============================] - 2s 459us/step - loss: 7.1710 - acc: 0.1158\n",
      "Epoch 12/20\n",
      "3427/3427 [==============================] - 2s 452us/step - loss: 7.1713 - acc: 0.1176\n",
      "Epoch 13/20\n",
      "3427/3427 [==============================] - 2s 457us/step - loss: 7.1680 - acc: 0.1188\n",
      "Epoch 14/20\n",
      "3427/3427 [==============================] - 2s 454us/step - loss: 7.1691 - acc: 0.1144\n",
      "Epoch 15/20\n",
      "3427/3427 [==============================] - 2s 452us/step - loss: 7.1697 - acc: 0.1083\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3427/3427 [==============================] - 1s 392us/step - loss: 7.1672 - acc: 0.1217\n",
      "Epoch 17/20\n",
      "3427/3427 [==============================] - 1s 376us/step - loss: 7.1663 - acc: 0.1103\n",
      "Epoch 18/20\n",
      "3427/3427 [==============================] - 1s 380us/step - loss: 7.1634 - acc: 0.1188\n",
      "Epoch 19/20\n",
      "3427/3427 [==============================] - 2s 503us/step - loss: 7.1653 - acc: 0.1135\n",
      "Epoch 20/20\n",
      "3427/3427 [==============================] - 2s 457us/step - loss: 7.1646 - acc: 0.1223\n",
      "381/381 [==============================] - 2s 4ms/step\n",
      "Epoch 1/20\n",
      "3428/3428 [==============================] - 6s 2ms/step - loss: 10.2314 - acc: 0.0916\n",
      "Epoch 2/20\n",
      "3428/3428 [==============================] - 2s 670us/step - loss: 9.6856 - acc: 0.0960 1s - loss: 9\n",
      "Epoch 3/20\n",
      "3428/3428 [==============================] - 2s 500us/step - loss: 9.6213 - acc: 0.0960\n",
      "Epoch 4/20\n",
      "3428/3428 [==============================] - 2s 603us/step - loss: 9.5993 - acc: 0.0980\n",
      "Epoch 5/20\n",
      "3428/3428 [==============================] - 2s 521us/step - loss: 9.5846 - acc: 0.1050\n",
      "Epoch 6/20\n",
      "3428/3428 [==============================] - 2s 672us/step - loss: 9.5805 - acc: 0.1062\n",
      "Epoch 7/20\n",
      "3428/3428 [==============================] - 2s 584us/step - loss: 9.5762 - acc: 0.1044\n",
      "Epoch 8/20\n",
      "3428/3428 [==============================] - 2s 579us/step - loss: 9.5734 - acc: 0.1053\n",
      "Epoch 9/20\n",
      "3428/3428 [==============================] - ETA: 0s - loss: 9.5657 - acc: 0.113 - 2s 644us/step - loss: 9.5615 - acc: 0.1135\n",
      "Epoch 10/20\n",
      "3428/3428 [==============================] - 2s 523us/step - loss: 9.5686 - acc: 0.1135\n",
      "Epoch 11/20\n",
      "3428/3428 [==============================] - 2s 537us/step - loss: 9.5638 - acc: 0.1074\n",
      "Epoch 12/20\n",
      "3428/3428 [==============================] - 2s 542us/step - loss: 9.5654 - acc: 0.1164\n",
      "Epoch 13/20\n",
      "3428/3428 [==============================] - 2s 540us/step - loss: 9.5626 - acc: 0.1088\n",
      "Epoch 14/20\n",
      "3428/3428 [==============================] - 2s 567us/step - loss: 9.5617 - acc: 0.1094\n",
      "Epoch 15/20\n",
      "3428/3428 [==============================] - 2s 521us/step - loss: 9.5597 - acc: 0.1161\n",
      "Epoch 16/20\n",
      "3428/3428 [==============================] - 2s 534us/step - loss: 9.5667 - acc: 0.1027\n",
      "Epoch 17/20\n",
      "3428/3428 [==============================] - 2s 532us/step - loss: 9.5620 - acc: 0.1094\n",
      "Epoch 18/20\n",
      "3428/3428 [==============================] - 2s 616us/step - loss: 9.5613 - acc: 0.1100\n",
      "Epoch 19/20\n",
      "3428/3428 [==============================] - 2s 595us/step - loss: 9.5605 - acc: 0.1144\n",
      "Epoch 20/20\n",
      "3428/3428 [==============================] - 2s 544us/step - loss: 9.5649 - acc: 0.1126 1s - l\n",
      "380/380 [==============================] - 2s 5ms/step\n",
      "Epoch 1/20\n",
      "3428/3428 [==============================] - 6s 2ms/step - loss: 4.9243 - acc: 0.0896\n",
      "Epoch 2/20\n",
      "3428/3428 [==============================] - 2s 648us/step - loss: 2.8592 - acc: 0.1187\n",
      "Epoch 3/20\n",
      "3428/3428 [==============================] - 2s 632us/step - loss: 2.7514 - acc: 0.1298\n",
      "Epoch 4/20\n",
      "3428/3428 [==============================] - 2s 695us/step - loss: 2.7002 - acc: 0.1313\n",
      "Epoch 5/20\n",
      "3428/3428 [==============================] - 2s 618us/step - loss: 2.6778 - acc: 0.1319\n",
      "Epoch 6/20\n",
      "3428/3428 [==============================] - 3s 813us/step - loss: 2.6691 - acc: 0.1313\n",
      "Epoch 7/20\n",
      "3428/3428 [==============================] - 2s 704us/step - loss: 2.6635 - acc: 0.1260\n",
      "Epoch 8/20\n",
      "3428/3428 [==============================] - 3s 779us/step - loss: 2.6611 - acc: 0.1327\n",
      "Epoch 9/20\n",
      "3428/3428 [==============================] - 2s 699us/step - loss: 2.6594 - acc: 0.1281\n",
      "Epoch 10/20\n",
      "3428/3428 [==============================] - 3s 745us/step - loss: 2.6571 - acc: 0.1324\n",
      "Epoch 11/20\n",
      "3428/3428 [==============================] - 2s 655us/step - loss: 2.6580 - acc: 0.1330\n",
      "Epoch 12/20\n",
      "3428/3428 [==============================] - 2s 637us/step - loss: 2.6567 - acc: 0.1339\n",
      "Epoch 13/20\n",
      "3428/3428 [==============================] - 2s 611us/step - loss: 2.6568 - acc: 0.1304\n",
      "Epoch 14/20\n",
      "3428/3428 [==============================] - 2s 592us/step - loss: 2.6560 - acc: 0.1269\n",
      "Epoch 15/20\n",
      "3428/3428 [==============================] - 2s 675us/step - loss: 2.6551 - acc: 0.1330\n",
      "Epoch 16/20\n",
      "3428/3428 [==============================] - 2s 589us/step - loss: 2.6565 - acc: 0.1307\n",
      "Epoch 17/20\n",
      "3428/3428 [==============================] - 2s 552us/step - loss: 2.6548 - acc: 0.1301\n",
      "Epoch 18/20\n",
      "3428/3428 [==============================] - 2s 622us/step - loss: 2.6547 - acc: 0.1263\n",
      "Epoch 19/20\n",
      "3428/3428 [==============================] - 2s 664us/step - loss: 2.6531 - acc: 0.1339\n",
      "Epoch 20/20\n",
      "3428/3428 [==============================] - 2s 569us/step - loss: 2.6536 - acc: 0.1348 0s - loss: 2.6579 - acc: 0.1\n",
      "380/380 [==============================] - 1s 4ms/step\n",
      "Baseline: 11.63% (1.54%)\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(24, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=20, batch_size=5, verbose=1)\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3808/3808 [==============================] - 3s 850us/step - loss: 13.4316 - acc: 0.0349\n",
      "Epoch 2/20\n",
      "3808/3808 [==============================] - 0s 65us/step - loss: 8.2649 - acc: 0.0972\n",
      "Epoch 3/20\n",
      "3808/3808 [==============================] - 0s 66us/step - loss: 4.5884 - acc: 0.0930\n",
      "Epoch 4/20\n",
      "3808/3808 [==============================] - 0s 67us/step - loss: 3.2847 - acc: 0.1045\n",
      "Epoch 5/20\n",
      "3808/3808 [==============================] - 0s 64us/step - loss: 3.1690 - acc: 0.1045\n",
      "Epoch 6/20\n",
      "3808/3808 [==============================] - 0s 67us/step - loss: 3.1275 - acc: 0.1129\n",
      "Epoch 7/20\n",
      "3808/3808 [==============================] - 0s 65us/step - loss: 3.1034 - acc: 0.1179\n",
      "Epoch 8/20\n",
      "3808/3808 [==============================] - 0s 64us/step - loss: 3.0954 - acc: 0.1124\n",
      "Epoch 9/20\n",
      "3808/3808 [==============================] - 0s 65us/step - loss: 3.0863 - acc: 0.1155\n",
      "Epoch 10/20\n",
      "3808/3808 [==============================] - 0s 67us/step - loss: 3.0775 - acc: 0.1182\n",
      "Epoch 11/20\n",
      "3808/3808 [==============================] - 0s 66us/step - loss: 3.0727 - acc: 0.1247\n",
      "Epoch 12/20\n",
      "3808/3808 [==============================] - 0s 70us/step - loss: 3.0716 - acc: 0.1119\n",
      "Epoch 13/20\n",
      "3808/3808 [==============================] - 0s 65us/step - loss: 3.0615 - acc: 0.1179\n",
      "Epoch 14/20\n",
      "3808/3808 [==============================] - 0s 68us/step - loss: 3.0592 - acc: 0.1242\n",
      "Epoch 15/20\n",
      "3808/3808 [==============================] - 0s 68us/step - loss: 3.0565 - acc: 0.1266\n",
      "Epoch 16/20\n",
      "3808/3808 [==============================] - 0s 71us/step - loss: 3.0506 - acc: 0.1124\n",
      "Epoch 17/20\n",
      "3808/3808 [==============================] - 0s 72us/step - loss: 3.0490 - acc: 0.1203\n",
      "Epoch 18/20\n",
      "3808/3808 [==============================] - 0s 103us/step - loss: 3.0511 - acc: 0.1176 0s - loss: 3.0520 - acc: 0.117\n",
      "Epoch 19/20\n",
      "3808/3808 [==============================] - 0s 116us/step - loss: 3.0461 - acc: 0.1216\n",
      "Epoch 20/20\n",
      "3808/3808 [==============================] - 0s 101us/step - loss: 3.0466 - acc: 0.1247\n",
      "<keras.callbacks.History object at 0x1a47547cf8>\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "ll = model.fit(X, dummy_y, 32, 20, verbose=1)\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(n_prev, 1), return_sequences=True))\n",
    "model.add(LSTM(128, input_shape=(n_prev, 1), return_sequences=True))\n",
    "model.add(LSTM(64, input_shape=(n_prev, 1), return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('linear'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "filepath=\"./Checkpoints/checkpoint_model_{epoch:02d}.hdf5\"\n",
    "model_save_callback = ModelCheckpoint(filepath, monitor='val_acc', \n",
    "                                      verbose=1, save_best_only=False, \n",
    "                                      mode='auto', period=5)\n",
    "\n",
    "# network_input = X\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(\n",
    "#         512,\n",
    "#         input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "#         return_sequences=True\n",
    "#     ))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(LSTM(512, return_sequences=True))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(LSTM(512))\n",
    "# model.add(Dense(256))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(n_vocab))\n",
    "# model.add(Activation('softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train your model.\n",
    "It might take a while, I was waiting for 1 hour with just 5 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3802/3802 [==============================] - 12s 3ms/step - loss: 0.0540\n",
      "Epoch 2/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0437\n",
      "Epoch 3/50\n",
      "3802/3802 [==============================] - 6s 2ms/step - loss: 0.0425\n",
      "Epoch 4/50\n",
      "3802/3802 [==============================] - 6s 2ms/step - loss: 0.0416\n",
      "Epoch 5/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0408\n",
      "Epoch 6/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0407\n",
      "Epoch 7/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0404\n",
      "Epoch 8/50\n",
      "3802/3802 [==============================] - 9s 2ms/step - loss: 0.0398\n",
      "Epoch 9/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0398\n",
      "Epoch 10/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0396\n",
      "Epoch 11/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0395\n",
      "Epoch 12/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0394\n",
      "Epoch 13/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0391\n",
      "Epoch 14/50\n",
      "3802/3802 [==============================] - 10s 3ms/step - loss: 0.0395\n",
      "Epoch 15/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0393\n",
      "Epoch 16/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0393\n",
      "Epoch 17/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0392\n",
      "Epoch 18/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0390\n",
      "Epoch 19/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0386\n",
      "Epoch 20/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0388\n",
      "Epoch 21/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0386\n",
      "Epoch 22/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0386\n",
      "Epoch 23/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0386\n",
      "Epoch 24/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0385\n",
      "Epoch 25/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0382\n",
      "Epoch 26/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0384\n",
      "Epoch 27/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0382\n",
      "Epoch 28/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0384\n",
      "Epoch 29/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0381\n",
      "Epoch 30/50\n",
      "3802/3802 [==============================] - 11s 3ms/step - loss: 0.0382\n",
      "Epoch 31/50\n",
      "3802/3802 [==============================] - 9s 2ms/step - loss: 0.0380\n",
      "Epoch 32/50\n",
      "3802/3802 [==============================] - 9s 2ms/step - loss: 0.0380\n",
      "Epoch 33/50\n",
      "3802/3802 [==============================] - 10s 3ms/step - loss: 0.0378\n",
      "Epoch 34/50\n",
      "3802/3802 [==============================] - 9s 2ms/step - loss: 0.0377\n",
      "Epoch 35/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0379\n",
      "Epoch 36/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0380\n",
      "Epoch 37/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0379\n",
      "Epoch 38/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0375\n",
      "Epoch 39/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0375\n",
      "Epoch 40/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0377\n",
      "Epoch 41/50\n",
      "3802/3802 [==============================] - 9s 2ms/step - loss: 0.0375\n",
      "Epoch 42/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0372\n",
      "Epoch 43/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0373\n",
      "Epoch 44/50\n",
      "3802/3802 [==============================] - 9s 2ms/step - loss: 0.0370\n",
      "Epoch 45/50\n",
      "3802/3802 [==============================] - 8s 2ms/step - loss: 0.0370\n",
      "Epoch 46/50\n",
      "3802/3802 [==============================] - 9s 2ms/step - loss: 0.0369\n",
      "Epoch 47/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0367\n",
      "Epoch 48/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0366\n",
      "Epoch 49/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0365\n",
      "Epoch 50/50\n",
      "3802/3802 [==============================] - 7s 2ms/step - loss: 0.0362\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "ll = model.fit(np.array(X), np.array(y), 32, 50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(np.array(X_test))\n",
    "prediction = np.squeeze(prediction)\n",
    "prediction = np.squeeze(scaler.inverse_transform(prediction.reshape(-1,1)))\n",
    "prediction = [int(i) for i in prediction]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your result to new midi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 38, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 39, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 42, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 38, 38, 39, 37, 37, 37, 39, 40, 40, 40, 39, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 42, 40, 38, 40, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 38, 38, 38, 39, 37, 37, 37, 39, 40, 40, 40, 39, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 38, 40, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 38, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 39, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 38, 38, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 38, 41, 38, 42, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 38, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 39, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 37, 37, 37, 38, 40, 37, 38, 37, 39, 39, 39, 41, 38, 41, 43, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 38, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 38, 41, 38, 42, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 39, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 38, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 39, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 39, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 38, 41, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 39, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 38, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 38, 38, 38, 39, 37, 37, 37, 39, 40, 40, 40, 39, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 42, 40, 38, 40, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 38, 41, 38, 42, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 38, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 39, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 42, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 38, 38, 39, 37, 37, 37, 39, 40, 40, 40, 39, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 42, 40, 38, 40, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 38, 38, 38, 39, 37, 37, 37, 39, 40, 40, 40, 39, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 38, 40, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 37, 37, 37, 38, 40, 37, 38, 37, 39, 39, 39, 41, 38, 41, 43, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 38, 41, 38, 42, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 38, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 39, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 37, 37, 37, 38, 40, 37, 38, 37, 39, 39, 39, 41, 38, 41, 43, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 38, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 41, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 38, 37, 37, 38, 39, 37, 37, 37, 39, 40, 40, 40, 39, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 42, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 38, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 38, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 41, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 38, 37, 38, 38, 39, 37, 37, 37, 39, 40, 40, 40, 39, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 38, 38, 39, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 41, 40, 38, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 41, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 38, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 41, 40, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 39, 37, 37, 39, 37, 37, 38, 39, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37, 37, 37, 38, 37, 37, 39, 37, 37, 39, 38, 37, 37, 37, 40, 40, 39, 40, 38, 42, 41, 37, 37, 37, 37]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)\n",
    "mid = MidiFile()\n",
    "track = MidiTrack()\n",
    "t = 0\n",
    "for note in prediction:\n",
    "    # 147 means note_on\n",
    "    # 67 is velosity\n",
    "    note = np.asarray([147, note, 67])\n",
    "    bytes = note.astype(int)\n",
    "    msg = Message.from_bytes(bytes[0:3])\n",
    "    t += 1\n",
    "    msg.time = t\n",
    "    track.append(msg)\n",
    "mid.tracks.append(track)\n",
    "mid.save('20_epoch_8_note_seq_CE.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just listen to it. The result is surreal!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
